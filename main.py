import os
import pandas as pd
from preprocessing import clean_text, correct_typos
from feature_extraction import get_tfidf_features
from classification import train_and_evaluate

# Veriyi yÃ¼kle
#df = load_bbc_dataset('datasets/bbc')
df = pd.read_csv("datasets/bbc-text.csv")

# Temizleme
df['cleaned_text'] = df['text'].apply(clean_text)

# YazÄ±m dÃ¼zeltme iÃ§in vocab oluÅŸtur
import nltk
nltk.download('words')
from nltk.corpus import words

vocabulary = set(words.words())

comparison_df = df[['text']].copy()
comparison_df['original_cleaned'] = df['text'].apply(clean_text)

all_changes = []

# TemizlenmiÅŸ metinlerin Ã¼zerinden geÃ§erek dÃ¼zeltmeleri uygula
corrected_texts = []
for doc in comparison_df['text']:
    corrected_raw, changes = correct_typos(doc, vocabulary, return_changes=True)
    
    # ArdÄ±ndan dÃ¼zeltmiÅŸ metni clean_text() ile temizle
    cleaned_corrected = clean_text(corrected_raw)
    
    corrected_texts.append(cleaned_corrected)
    all_changes.extend(changes)

# DataFrame'e gÃ¼ncel metinleri kaydet
df['cleaned_text'] = corrected_texts

# Sadece benzersiz dÃ¼zeltmeleri yazdÄ±r
unique_changes = list(set(all_changes))
print("\nâœï¸ DÃ¼zeltilen kelimeler (hatalÄ± -> doÄŸru):")
for wrong, correct in unique_changes:
    print(f"{wrong} â¡ï¸ {correct}")

# YazÄ±m dÃ¼zeltme sÄ±rasÄ±nda yapÄ±lan deÄŸiÅŸiklikleri say
num_changes = sum(1 for orig, corrected in zip(comparison_df['original_cleaned'], comparison_df['cleaned_text']) if orig != corrected)
print(f"\nğŸ§  Toplamda {num_changes} metinde yazÄ±m dÃ¼zeltmesi yapÄ±ldÄ±.")

# TF-IDF Ã¶zellikleri
X, vectorizer = get_tfidf_features(df['cleaned_text'])

# Etiketler
y = df['category']

# Modeli eÄŸit ve deÄŸerlendir
print("ğŸ¤– Model eÄŸitiliyor ve deÄŸerlendiriliyor...")
model = train_and_evaluate(X, y)

def load_bbc_dataset(folder_path):
    categories = os.listdir(folder_path)
    data = []

    for category in categories:
        category_path = os.path.join(folder_path, category)
        for filename in os.listdir(category_path):
            file_path = os.path.join(category_path, filename)
            with open(file_path, 'r', encoding='latin1') as f:
                text = f.read()
                data.append({'category': category, 'text': text})
    
    return pd.DataFrame(data)

df = load_bbc_dataset('datasets/bbc')
print(df.head())
# The above code loads the BBC dataset from a specified folder path, where each category is stored in a separate subfolder.
# It reads the text files and creates a DataFrame with two columns: 'category' and 'text'.